{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMS Spam Classifier using NLP\n",
    "\n",
    "This notebook demonstrates how to build a machine learning model to classify SMS messages as 'spam' or 'ham' (legitimate messages) using Natural Language Processing techniques.\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "- **Objective**: Build an SMS spam detection system\n",
    "- **Dataset**: SMS Spam Collection Dataset\n",
    "- **Algorithm**: Multinomial Naive Bayes\n",
    "- **Features**: TF-IDF (Term Frequency-Inverse Document Frequency) vectors\n",
    "- **Text Processing**: Cleaning, stemming, stopword removal\n",
    "\n",
    "## Table of Contents\n",
    "1. [Import Libraries](#1-import-libraries)\n",
    "2. [Load Dataset](#2-load-dataset)\n",
    "3. [Data Exploration](#3-data-exploration)\n",
    "4. [Data Preprocessing](#4-data-preprocessing)\n",
    "5. [Feature Extraction](#5-feature-extraction)\n",
    "6. [Model Training](#6-model-training)\n",
    "7. [Model Evaluation](#7-model-evaluation)\n",
    "8. [Interactive Prediction](#8-interactive-prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries\n",
    "\n",
    "First, let's import all the necessary libraries for our SMS spam classification project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "\n",
    "# Machine learning libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Natural Language Processing\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Text processing\n",
    "import string\n",
    "\n",
    "# Data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Download NLTK data (run once)\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Dataset\n",
    "\n",
    "The dataset should be named `spam.csv` and placed in the same directory as this notebook. The dataset contains two main columns:\n",
    "- `v1`: Label (ham/spam)\n",
    "- `v2`: SMS message text\n",
    "\n",
    "**Dataset Source**: You can download the SMS Spam Collection Dataset from [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/sms+spam+collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "try:\n",
    "    df = pd.read_csv('spam.csv', encoding='latin-1')\n",
    "    df = df.iloc[:, :2]  # Select the first two columns\n",
    "    df.columns = ['label', 'message']\n",
    "    print(\"Dataset loaded successfully. First 5 rows:\")\n",
    "    print(df.head())\n",
    "    print(\"\\nDataset shape:\", df.shape)\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'spam.csv' not found. Please ensure the file is in the same directory.\")\n",
    "    print(\"You can download it from: https://archive.ics.uci.edu/ml/datasets/sms+spam+collection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Exploration\n",
    "\n",
    "Let's explore our dataset to understand the distribution of spam vs ham messages and get insights into our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Data Exploration ---\")\n",
    "print(\"Class distribution:\")\n",
    "print(df['label'].value_counts())\n",
    "\n",
    "# Calculate percentages\n",
    "percentages = df['label'].value_counts(normalize=True) * 100\n",
    "print(\"\\nClass distribution (percentages):\")\n",
    "print(percentages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the class distribution\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Count plot\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.countplot(x='label', data=df, palette='viridis')\n",
    "plt.title('Distribution of Ham vs. Spam Messages')\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Pie chart\n",
    "plt.subplot(1, 2, 2)\n",
    "df['label'].value_counts().plot(kind='pie', autopct='%1.1f%%', colors=['lightblue', 'lightcoral'])\n",
    "plt.title('Ham vs. Spam Distribution')\n",
    "plt.ylabel('')  # Remove default ylabel\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore message lengths\n",
    "df['message_length'] = df['message'].str.len()\n",
    "\n",
    "print(\"Message length statistics:\")\n",
    "print(df.groupby('label')['message_length'].describe())\n",
    "\n",
    "# Plot message length distribution\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "df[df['label'] == 'ham']['message_length'].hist(bins=30, alpha=0.7, label='Ham', color='blue')\n",
    "df[df['label'] == 'spam']['message_length'].hist(bins=30, alpha=0.7, label='Spam', color='red')\n",
    "plt.xlabel('Message Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Message Length Distribution')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.boxplot(x='label', y='message_length', data=df, palette='Set2')\n",
    "plt.title('Message Length by Label')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing\n",
    "\n",
    "Text preprocessing is crucial for NLP tasks. We'll perform the following steps:\n",
    "1. Convert labels to numerical format\n",
    "2. Clean text data (lowercase, remove punctuation, remove stopwords, stemming)\n",
    "\n",
    "### 4.1 Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to numerical format (0 for ham, 1 for spam)\n",
    "le = LabelEncoder()\n",
    "df['label'] = le.fit_transform(df['label'])\n",
    "print(\"Labels encoded. First 5 rows with numerical labels:\")\n",
    "print(df[['label', 'message']].head())\n",
    "print(\"\\nLabel mapping:\")\n",
    "print(\"0 -> Ham (Legitimate)\")\n",
    "print(\"1 -> Spam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Text Cleaning Function\n",
    "\n",
    "We'll create a comprehensive text cleaning function that:\n",
    "- Converts text to lowercase\n",
    "- Removes punctuation\n",
    "- Removes stopwords (common words like 'the', 'a', 'is')\n",
    "- Applies stemming (reduces words to their root form)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Comprehensive text cleaning function.\n",
    "    \n",
    "    Steps:\n",
    "    1. Convert to lowercase\n",
    "    2. Remove punctuation\n",
    "    3. Remove stopwords\n",
    "    4. Apply stemming\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text to clean\n",
    "    \n",
    "    Returns:\n",
    "        str: Cleaned text\n",
    "    \"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text = ''.join([char for char in text if char not in string.punctuation])\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = stopwords.words('english')\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "    \n",
    "    # Apply stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    text = ' '.join([stemmer.stem(word) for word in text.split()])\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Test the function with a sample message\n",
    "sample_text = \"Hello! This is a sample message with punctuation, stopwords, and words that need stemming.\"\n",
    "print(\"Original text:\")\n",
    "print(sample_text)\n",
    "print(\"\\nCleaned text:\")\n",
    "print(clean_text(sample_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the cleaning function to all messages\n",
    "print(\"Cleaning text data...\")\n",
    "df['cleaned_message'] = df['message'].apply(clean_text)\n",
    "print(\"Text cleaning complete. First 5 cleaned messages:\")\n",
    "print(df[['message', 'cleaned_message']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare original vs cleaned message lengths\n",
    "df['cleaned_length'] = df['cleaned_message'].str.len()\n",
    "print(\"Length comparison (original vs cleaned):\")\n",
    "print(f\"Average original length: {df['message_length'].mean():.2f}\")\n",
    "print(f\"Average cleaned length: {df['cleaned_length'].mean():.2f}\")\n",
    "print(f\"Average reduction: {((df['message_length'].mean() - df['cleaned_length'].mean()) / df['message_length'].mean()) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Extraction\n",
    "\n",
    "We'll use **TF-IDF (Term Frequency-Inverse Document Frequency)** to convert our cleaned text into numerical features that machine learning algorithms can understand.\n",
    "\n",
    "### What is TF-IDF?\n",
    "- **Term Frequency (TF)**: How often a word appears in a document\n",
    "- **Inverse Document Frequency (IDF)**: How rare or common a word is across all documents\n",
    "- **TF-IDF**: Combines both to give higher weights to words that are frequent in a document but rare across the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Converting text to TF-IDF vectors...\")\n",
    "\n",
    "# Initialize TF-IDF vectorizer\n",
    "tfidf = TfidfVectorizer(max_features=5000)  # Limit to top 5000 features\n",
    "\n",
    "# Transform cleaned messages to TF-IDF vectors\n",
    "X = tfidf.fit_transform(df['cleaned_message'])\n",
    "y = df['label']\n",
    "\n",
    "print(\"TF-IDF vectorization complete.\")\n",
    "print(f\"Shape of feature matrix: {X.shape}\")\n",
    "print(f\"Number of samples: {X.shape[0]}\")\n",
    "print(f\"Number of features: {X.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show top features (words) learned by TF-IDF\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "print(\"Sample of TF-IDF features (words):\")\n",
    "print(feature_names[:20])  # Show first 20 features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Training\n",
    "\n",
    "We'll use **Multinomial Naive Bayes** classifier, which is particularly effective for text classification tasks.\n",
    "\n",
    "### Why Naive Bayes?\n",
    "- Works well with text data\n",
    "- Fast training and prediction\n",
    "- Good performance on spam detection\n",
    "- Handles high-dimensional sparse data (like TF-IDF vectors) well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Splitting data and training the Multinomial Naive Bayes model...\")\n",
    "\n",
    "# Split data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
    "print(f\"Testing set size: {X_test.shape[0]} samples\")\n",
    "\n",
    "# Initialize and train the Naive Bayes classifier\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "print(\"Model training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Evaluation\n",
    "\n",
    "Let's evaluate our model's performance using various metrics:\n",
    "- **Accuracy**: Overall correctness\n",
    "- **Precision**: Of all predicted spam, how many were actually spam?\n",
    "- **Recall**: Of all actual spam, how many did we correctly identify?\n",
    "- **F1-Score**: Harmonic mean of precision and recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Model Evaluation ---\")\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy:  {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "print(f\"Precision: {precision:.4f} ({precision*100:.2f}%)\")\n",
    "print(f\"Recall:    {recall:.4f} ({recall*100:.2f}%)\")\n",
    "print(f\"F1-Score:  {f1:.4f} ({f1*100:.2f}%)\")\n",
    "\n",
    "print(\"\\n--- Metric Interpretations ---\")\n",
    "print(f\"✓ Out of 100 messages, {accuracy*100:.0f} are classified correctly\")\n",
    "print(f\"✓ Out of 100 predicted spam messages, {precision*100:.0f} are actually spam\")\n",
    "print(f\"✓ Out of 100 actual spam messages, {recall*100:.0f} are correctly identified\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix Visualization\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['HAM', 'SPAM'], yticklabels=['HAM', 'SPAM'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()\n",
    "\n",
    "# Interpret confusion matrix\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "print(\"\\n--- Confusion Matrix Breakdown ---\")\n",
    "print(f\"True Negatives (Ham → Ham): {tn}\")\n",
    "print(f\"False Positives (Ham → Spam): {fp}\")\n",
    "print(f\"False Negatives (Spam → Ham): {fn}\")\n",
    "print(f\"True Positives (Spam → Spam): {tp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance Analysis\n",
    "# Get the most important features for spam classification\n",
    "feature_log_prob = model.feature_log_prob_\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "\n",
    "# Calculate feature importance for spam class (class 1)\n",
    "spam_features = feature_log_prob[1] - feature_log_prob[0]\n",
    "top_spam_indices = spam_features.argsort()[-20:][::-1]  # Top 20 spam indicators\n",
    "\n",
    "print(\"--- Top 20 Words Indicating SPAM ---\")\n",
    "for i, idx in enumerate(top_spam_indices, 1):\n",
    "    print(f\"{i:2d}. {feature_names[idx]}\")\n",
    "\n",
    "# Top ham indicators (least likely to be spam)\n",
    "ham_features = feature_log_prob[0] - feature_log_prob[1]\n",
    "top_ham_indices = ham_features.argsort()[-20:][::-1]  # Top 20 ham indicators\n",
    "\n",
    "print(\"\\n--- Top 20 Words Indicating HAM ---\")\n",
    "for i, idx in enumerate(top_ham_indices, 1):\n",
    "    print(f\"{i:2d}. {feature_names[idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Interactive Prediction\n",
    "\n",
    "Now let's create a function to test our model with new messages and see it in action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sms_spam(message, model, tfidf_vectorizer, label_encoder):\n",
    "    \"\"\"\n",
    "    Predict whether a single SMS message is spam or ham.\n",
    "    \n",
    "    Args:\n",
    "        message (str): SMS message to classify\n",
    "        model: Trained classifier\n",
    "        tfidf_vectorizer: Fitted TF-IDF vectorizer\n",
    "        label_encoder: Fitted label encoder\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (prediction_label, confidence_score)\n",
    "    \"\"\"\n",
    "    # Clean the message\n",
    "    cleaned_message = clean_text(message)\n",
    "    \n",
    "    # Vectorize the message\n",
    "    vectorized_message = tfidf_vectorizer.transform([cleaned_message])\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction = model.predict(vectorized_message)[0]\n",
    "    prediction_proba = model.predict_proba(vectorized_message)[0]\n",
    "    \n",
    "    # Get label and confidence\n",
    "    prediction_label = label_encoder.inverse_transform([prediction])[0]\n",
    "    confidence = max(prediction_proba) * 100\n",
    "    \n",
    "    return prediction_label, confidence\n",
    "\n",
    "print(\"--- Interactive SMS Spam Classifier ---\")\n",
    "print(\"Testing the model with example messages:\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test messages\n",
    "test_messages = [\n",
    "    \"Get your free phone now! Call 123456.\",\n",
    "    \"Hey, what are you doing later?\",\n",
    "    \"URGENT! You have won $1000! Click here now!\",\n",
    "    \"Can you pick up some milk on your way home?\",\n",
    "    \"FREE entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005.\",\n",
    "    \"I'll call you tomorrow morning.\",\n",
    "    \"Congratulations! You've won a lottery! Send your bank details now!\",\n",
    "    \"Meeting at 3 PM today. Don't forget!\"\n",
    "]\n",
    "\n",
    "for i, message in enumerate(test_messages, 1):\n",
    "    prediction, confidence = predict_sms_spam(message, model, tfidf, le)\n",
    "    print(f\"{i}. Message: '{message}'\")\n",
    "    print(f\"   → Prediction: {prediction.upper()} (Confidence: {confidence:.1f}%)\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive input (uncomment to test with your own messages)\n",
    "# while True:\n",
    "#     user_message = input(\"\\nEnter an SMS message to classify (or 'quit' to stop): \")\n",
    "#     if user_message.lower() == 'quit':\n",
    "#         break\n",
    "#     \n",
    "#     prediction, confidence = predict_sms_spam(user_message, model, tfidf, le)\n",
    "#     print(f\"Prediction: {prediction.upper()} (Confidence: {confidence:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Key Takeaways\n",
    "\n",
    "### Model Performance\n",
    "- **High Accuracy**: The model achieves excellent accuracy in classifying SMS messages\n",
    "- **Perfect Precision**: When the model predicts spam, it's highly reliable\n",
    "- **Good Recall**: The model catches most spam messages, though some may slip through\n",
    "\n",
    "### Key Features of Spam Messages\n",
    "Based on our analysis, spam messages typically contain:\n",
    "- Words related to free offers, prizes, and urgent calls to action\n",
    "- Contact information (phone numbers, websites)\n",
    "- Financial terms and promotional language\n",
    "\n",
    "### Technical Approach\n",
    "1. **Text Preprocessing**: Cleaned data by removing noise and normalizing text\n",
    "2. **Feature Engineering**: Used TF-IDF to convert text to numerical features\n",
    "3. **Model Selection**: Multinomial Naive Bayes proved effective for this task\n",
    "4. **Evaluation**: Comprehensive metrics to assess model performance\n",
    "\n",
    "### Potential Improvements\n",
    "- Try other algorithms (SVM, Random Forest, Deep Learning)\n",
    "- Feature engineering (n-grams, word embeddings)\n",
    "- Handle class imbalance with techniques like SMOTE\n",
    "- Cross-validation for more robust evaluation\n",
    "- Real-time model updating with new data\n",
    "\n",
    "### Usage Notes\n",
    "- This model works best with English SMS messages\n",
    "- Performance may vary with different types of spam over time\n",
    "- Regular retraining with new data is recommended for production use"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}